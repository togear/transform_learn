"""
Tiny Shakespeare æ•°æ®é›†å®Œæ•´æ”¶æ•›å®éªŒ
ä½¿ç”¨ Transformer æ¨¡å‹è¿›è¡Œå­—ç¬¦çº§è¯­è¨€å»ºæ¨¡çš„æ”¶æ•›æ€§åˆ†æ
åŒ…å«è¯¦ç»†çš„è®­ç»ƒç›‘æ§ã€å¯è§†åŒ–å’Œæ”¶æ•›æ€§æŒ‡æ ‡
"""

import os
import sys
import math
import time
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader
import matplotlib.pyplot as plt
import numpy as np
from tqdm import tqdm
import json
import argparse
from datetime import datetime

# æ·»åŠ  src ç›®å½•åˆ°è·¯å¾„
sys.path.append('src')
from model import make_transformer_lm
from train import TinyTextDataset


def setup_experiment(args):
    """è®¾ç½®å®éªŒç¯å¢ƒå’Œå‚æ•°"""
    print("="*80)
    print(" Tiny Shakespeare æ”¶æ•›å®éªŒ")
    print("="*80)

    # è®¾ç½®éšæœºç§å­ç¡®ä¿å¯å¤ç°æ€§
    torch.manual_seed(args.seed)
    np.random.seed(args.seed)

    # è®¾ç½®è®¾å¤‡
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"ğŸ“± ä½¿ç”¨è®¾å¤‡: {device}")
    if device.type == 'cuda':
        print(f"   GPU: {torch.cuda.get_device_name()}")
        print(f"   å†…å­˜: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB")

    # åˆ›å»ºå®éªŒç›®å½•
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    exp_dir = f"experiments/shakespeare_convergence_{timestamp}"
    os.makedirs(exp_dir, exist_ok=True)
    os.makedirs(f"{exp_dir}/plots", exist_ok=True)
    os.makedirs(f"{exp_dir}/models", exist_ok=True)

    print(f" å®éªŒç›®å½•: {exp_dir}")

    return device, exp_dir


def load_shakespeare_data(args):
    """åŠ è½½å’Œé¢„å¤„ç† Tiny Shakespeare æ•°æ®"""
    print("\n åŠ è½½ Tiny Shakespeare æ•°æ®é›†")
    print("-" * 50)

    data_path = "data/input.txt"
    if not os.path.exists(data_path):
        raise FileNotFoundError(
            f"æœªæ‰¾åˆ° Tiny Shakespeare æ•°æ®é›†ï¼\n"
            f"è¯·ç¡®ä¿æ–‡ä»¶å­˜åœ¨: {data_path}\n"
            f"å¯ä»¥è¿è¡Œ: curl -O https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"
        )

    # è¯»å–æ–‡æœ¬
    with open(data_path, 'r', encoding='utf-8') as f:
        text = f.read()

    print(f"  æ•°æ®ç»Ÿè®¡:")
    print(f"   æ€»å­—ç¬¦æ•°: {len(text):,}")
    print(f"   å”¯ä¸€å­—ç¬¦æ•°: {len(set(text))}")

    # æ•°æ®åˆ’åˆ†ï¼šè®­ç»ƒé›†(90%), éªŒè¯é›†(10%)
    split_idx = int(len(text) * 0.9)
    train_text = text[:split_idx]
    val_text = text[split_idx:]

    print(f"   è®­ç»ƒé›†: {len(train_text):,} å­—ç¬¦")
    print(f"   éªŒè¯é›†: {len(val_text):,} å­—ç¬¦")

    # åˆ›å»ºæ•°æ®é›†
    train_dataset = TinyTextDataset(train_text, seq_len=args.seq_len)
    val_dataset = TinyTextDataset(val_text, seq_len=args.seq_len)

    # åˆ›å»ºæ•°æ®åŠ è½½å™¨
    train_loader = DataLoader(
        train_dataset,
        batch_size=args.batch_size,
        shuffle=True,
        num_workers=0,
        pin_memory=torch.cuda.is_available()
    )

    val_loader = DataLoader(
        val_dataset,
        batch_size=args.batch_size,
        shuffle=False,
        num_workers=0,
        pin_memory=torch.cuda.is_available()
    )

    return train_dataset, val_dataset, train_loader, val_loader


def create_model(vocab_size, args, device):
    """åˆ›å»ºå’Œåˆå§‹åŒ–æ¨¡å‹"""
    print(f"\n  åˆ›å»º Transformer æ¨¡å‹")
    print("-" * 50)

    model = make_transformer_lm(
        vocab_size=vocab_size,
        d_model=args.d_model,
        num_heads=args.num_heads,
        d_ff=args.d_ff,
        num_layers=args.num_layers,
        dropout=args.dropout
    )

    model = model.to(device)

    # è®¡ç®—å‚æ•°é‡
    total_params = sum(p.numel() for p in model.parameters())
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)

    print(f" æ¨¡å‹æ¶æ„:")
    print(f"   æ€»å‚æ•°é‡: {total_params:,}")
    print(f"   å¯è®­ç»ƒå‚æ•°: {trainable_params:,}")
    print(f"   æ¨¡å‹å¤§å°: {total_params * 4 / 1e6:.2f} MB (float32)")

    return model


def setup_training(model, args):
    """è®¾ç½®è®­ç»ƒç»„ä»¶ï¼ˆä¼˜åŒ–å™¨ã€æŸå¤±å‡½æ•°ã€è°ƒåº¦å™¨ï¼‰"""
    print(f"\n  é…ç½®è®­ç»ƒç»„ä»¶")
    print("-" * 50)

    # ä¼˜åŒ–å™¨
    optimizer = optim.AdamW(
        model.parameters(),
        lr=args.lr,
        betas=(0.9, 0.95),  # GPT-3 çš„è®¾ç½®
        eps=1e-8,
        weight_decay=args.weight_decay
    )

    # æŸå¤±å‡½æ•°
    criterion = nn.CrossEntropyLoss()

    # å­¦ä¹ ç‡è°ƒåº¦å™¨ï¼ˆä½™å¼¦é€€ç« + warmupï¼‰
    def get_lr_scheduler(optimizer, warmup_steps, total_steps):
        def lr_lambda(step):
            if step < warmup_steps:
                # Warmup phase: çº¿æ€§å¢é•¿
                return step / warmup_steps
            else:
                # ä½™å¼¦é€€ç«
                progress = (step - warmup_steps) / (total_steps - warmup_steps)
                return 0.5 * (1 + math.cos(math.pi * progress))

        return optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)

    total_steps = args.epochs * args.steps_per_epoch if hasattr(args, 'steps_per_epoch') else args.epochs * 1000
    scheduler = get_lr_scheduler(optimizer, args.warmup_steps, total_steps)

    print(f"  è®­ç»ƒé…ç½®:")
    print(f"   ä¼˜åŒ–å™¨: AdamW")
    print(f"   åˆå§‹å­¦ä¹ ç‡: {args.lr}")
    print(f"   æƒé‡è¡°å‡: {args.weight_decay}")
    print(f"   Warmup æ­¥æ•°: {args.warmup_steps}")

    return optimizer, criterion, scheduler


class ConvergenceTracker:
    """æ”¶æ•›æ€§è·Ÿè¸ªå™¨"""

    def __init__(self, patience=10, min_delta=1e-4):
        self.patience = patience
        self.min_delta = min_delta
        self.best_loss = float('inf')
        self.wait = 0
        self.stopped_epoch = 0

        # è®°å½•å„ç§æŒ‡æ ‡
        self.train_losses = []
        self.val_losses = []
        self.train_ppls = []
        self.val_ppls = []
        self.learning_rates = []
        self.grad_norms = []
        self.step_times = []

    def update(self, train_loss, val_loss, train_ppl, val_ppl, lr, grad_norm, step_time):
        """æ›´æ–°è·Ÿè¸ªæŒ‡æ ‡"""
        self.train_losses.append(train_loss)
        self.val_losses.append(val_loss)
        self.train_ppls.append(train_ppl)
        self.val_ppls.append(val_ppl)
        self.learning_rates.append(lr)
        self.grad_norms.append(grad_norm)
        self.step_times.append(step_time)

        # æ£€æŸ¥æ˜¯å¦æœ‰æ”¹è¿›
        if val_loss < self.best_loss - self.min_delta:
            self.best_loss = val_loss
            self.wait = 0
            return False  # æœªæ”¶æ•›
        else:
            self.wait += 1
            return self.wait >= self.patience  # æ˜¯å¦è¾¾åˆ°æ—©åœæ¡ä»¶

    def get_convergence_metrics(self):
        """è®¡ç®—æ”¶æ•›æ€§æŒ‡æ ‡"""
        if len(self.val_losses) < 10:
            return {}

        # æœ€è¿‘ 10 ä¸ª epoch çš„æŸå¤±å˜åŒ–
        recent_val_losses = self.val_losses[-10:]
        loss_variance = np.var(recent_val_losses)
        loss_trend = np.polyfit(range(len(recent_val_losses)), recent_val_losses, 1)[0]

        # æŸå¤±å¹³æ»‘æ€§ï¼ˆç›¸é‚» epoch ä¹‹é—´çš„å¹³å‡å˜åŒ–ï¼‰
        loss_smoothness = np.mean([abs(self.val_losses[i] - self.val_losses[i-1])
                                  for i in range(1, len(self.val_losses))])

        return {
            'best_val_loss': self.best_loss,
            'final_val_loss': self.val_losses[-1],
            'loss_variance': loss_variance,
            'loss_trend': loss_trend,
            'loss_smoothness': loss_smoothness,
            'converged_at_epoch': len(self.val_losses) - self.wait if self.wait >= self.patience else None
        }


def train_epoch(model, train_loader, optimizer, criterion, device, epoch, tracker, scheduler=None):
    """è®­ç»ƒä¸€ä¸ª epoch"""
    model.train()
    total_loss = 0
    total_tokens = 0
    grad_norms = []

    start_time = time.time()
    pbar = tqdm(train_loader, desc=f"Epoch {epoch:3d}")

    for batch_idx, (x, y) in enumerate(pbar):
        x, y = x.to(device), y.to(device)
        batch_size = x.size(0)

        # å‰å‘ä¼ æ’­
        optimizer.zero_grad()
        logits = model(x)
        loss = criterion(logits.view(-1, logits.size(-1)), y.view(-1))

        # åå‘ä¼ æ’­
        loss.backward()

        # æ¢¯åº¦è£å‰ªå’Œè®°å½•
        grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
        grad_norms.append(grad_norm.item())

        optimizer.step()
        if scheduler:
            scheduler.step()

        # ç´¯ç§¯ç»Ÿè®¡
        total_loss += loss.item() * batch_size
        total_tokens += batch_size

        # æ›´æ–°è¿›åº¦æ¡
        current_lr = optimizer.param_groups[0]['lr']
        pbar.set_postfix({
            'loss': f'{loss.item():.4f}',
            'lr': f'{current_lr:.2e}',
            'grad_norm': f'{grad_norm.item():.3f}'
        })

    epoch_time = time.time() - start_time
    avg_loss = total_loss / total_tokens
    avg_grad_norm = np.mean(grad_norms)
    perplexity = math.exp(min(avg_loss, 20))

    return avg_loss, perplexity, avg_grad_norm, epoch_time


def validate(model, val_loader, criterion, device):
    """éªŒè¯æ¨¡å‹"""
    model.eval()
    total_loss = 0
    total_tokens = 0

    with torch.no_grad():
        for x, y in val_loader:
            x, y = x.to(device), y.to(device)
            batch_size = x.size(0)

            logits = model(x)
            loss = criterion(logits.view(-1, logits.size(-1)), y.view(-1))

            total_loss += loss.item() * batch_size
            total_tokens += batch_size

    avg_loss = total_loss / total_tokens
    perplexity = math.exp(min(avg_loss, 20))

    return avg_loss, perplexity


def generate_text(model, dataset, device, start_text="First Citizen:", max_len=200, temperature=0.8):
    """ç”Ÿæˆæ ·æœ¬æ–‡æœ¬"""
    model.eval()

    # å°†èµ·å§‹æ–‡æœ¬è½¬æ¢ä¸ºç´¢å¼•
    chars = [dataset.char2idx.get(c, 0) for c in start_text]
    input_seq = torch.tensor(chars, dtype=torch.long).unsqueeze(0).to(device)

    generated = start_text

    with torch.no_grad():
        for _ in range(max_len):
            if input_seq.size(1) > dataset.seq_len:
                input_seq = input_seq[:, -dataset.seq_len:]

            logits = model(input_seq)
            next_token_logits = logits[0, -1, :] / temperature

            probs = torch.softmax(next_token_logits, dim=-1)
            next_token = torch.multinomial(probs, num_samples=1)

            next_char = dataset.idx2char[next_token.item()]
            generated += next_char

            input_seq = torch.cat([input_seq, next_token.unsqueeze(0)], dim=1)

    return generated


def plot_convergence_analysis(tracker, exp_dir):
    """ç»˜åˆ¶æ”¶æ•›æ€§åˆ†æå›¾è¡¨"""
    epochs = range(1, len(tracker.train_losses) + 1)

    # åˆ›å»ºå¤šå­å›¾
    fig, axes = plt.subplots(2, 3, figsize=(18, 12))
    fig.suptitle('Tiny Shakespeare æ”¶æ•›æ€§åˆ†æ', fontsize=16, fontweight='bold')

    # 1. æŸå¤±æ›²çº¿
    axes[0, 0].plot(epochs, tracker.train_losses, 'b-', label='Training Loss', linewidth=2)
    axes[0, 0].plot(epochs, tracker.val_losses, 'r-', label='Validation Loss', linewidth=2)
    axes[0, 0].set_xlabel('Epoch')
    axes[0, 0].set_ylabel('Loss')
    axes[0, 0].set_title('è®­ç»ƒå’ŒéªŒè¯æŸå¤±')
    axes[0, 0].legend()
    axes[0, 0].grid(True, alpha=0.3)

    # 2. å›°æƒ‘åº¦æ›²çº¿
    axes[0, 1].plot(epochs, tracker.train_ppls, 'b-', label='Training PPL', linewidth=2)
    axes[0, 1].plot(epochs, tracker.val_ppls, 'r-', label='Validation PPL', linewidth=2)
    axes[0, 1].set_xlabel('Epoch')
    axes[0, 1].set_ylabel('Perplexity')
    axes[0, 1].set_title('å›°æƒ‘åº¦ (Perplexity)')
    axes[0, 1].legend()
    axes[0, 1].grid(True, alpha=0.3)
    axes[0, 1].set_yscale('log')

    # 3. å­¦ä¹ ç‡æ›²çº¿
    axes[0, 2].plot(epochs, tracker.learning_rates, 'g-', linewidth=2)
    axes[0, 2].set_xlabel('Epoch')
    axes[0, 2].set_ylabel('Learning Rate')
    axes[0, 2].set_title('å­¦ä¹ ç‡è°ƒåº¦')
    axes[0, 2].grid(True, alpha=0.3)
    axes[0, 2].set_yscale('log')

    # 4. æ¢¯åº¦èŒƒæ•°
    axes[1, 0].plot(epochs, tracker.grad_norms, 'purple', linewidth=2)
    axes[1, 0].set_xlabel('Epoch')
    axes[1, 0].set_ylabel('Gradient Norm')
    axes[1, 0].set_title('æ¢¯åº¦èŒƒæ•°')
    axes[1, 0].grid(True, alpha=0.3)

    # 5. è®­ç»ƒæ—¶é—´
    axes[1, 1].plot(epochs, tracker.step_times, 'orange', linewidth=2)
    axes[1, 1].set_xlabel('Epoch')
    axes[1, 1].set_ylabel('Time (seconds)')
    axes[1, 1].set_title('æ¯è½®è®­ç»ƒæ—¶é—´')
    axes[1, 1].grid(True, alpha=0.3)

    # 6. æ”¶æ•›æ€§æŒ‡æ ‡
    if len(tracker.val_losses) >= 10:
        # è®¡ç®—ç§»åŠ¨å¹³å‡ä»¥æ˜¾ç¤ºè¶‹åŠ¿
        window_size = min(10, len(tracker.val_losses) // 4)
        val_loss_ma = np.convolve(tracker.val_losses,
                                 np.ones(window_size)/window_size, mode='valid')
        ma_epochs = epochs[window_size-1:]

        axes[1, 2].plot(epochs, tracker.val_losses, 'r-', alpha=0.5, label='åŸå§‹éªŒè¯æŸå¤±')
        axes[1, 2].plot(ma_epochs, val_loss_ma, 'r-', linewidth=3, label=f'ç§»åŠ¨å¹³å‡ (çª—å£={window_size})')
        axes[1, 2].set_xlabel('Epoch')
        axes[1, 2].set_ylabel('Validation Loss')
        axes[1, 2].set_title('éªŒè¯æŸå¤±è¶‹åŠ¿')
        axes[1, 2].legend()
        axes[1, 2].grid(True, alpha=0.3)

    plt.tight_layout()

    # ä¿å­˜å›¾è¡¨
    plot_path = f"{exp_dir}/plots/convergence_analysis.png"
    plt.savefig(plot_path, dpi=300, bbox_inches='tight')
    print(f" æ”¶æ•›æ€§åˆ†æå›¾è¡¨å·²ä¿å­˜: {plot_path}")
    plt.close()


def save_experiment_results(args, tracker, model, dataset, exp_dir):
    """ä¿å­˜å®éªŒç»“æœ"""
    print(f"\n ä¿å­˜å®éªŒç»“æœ")
    print("-" * 50)

    # æ”¶æ•›æ€§æŒ‡æ ‡
    convergence_metrics = tracker.get_convergence_metrics()

    # å®éªŒé…ç½®å’Œç»“æœ
    results = {
        'experiment_config': {
            'dataset': 'tiny_shakespeare',
            'model_type': 'transformer_encoder_only',
            'vocab_size': dataset.vocab_size,
            'seq_len': args.seq_len,
            'd_model': args.d_model,
            'num_heads': args.num_heads,
            'd_ff': args.d_ff,
            'num_layers': args.num_layers,
            'dropout': args.dropout,
            'batch_size': args.batch_size,
            'learning_rate': args.lr,
            'weight_decay': args.weight_decay,
            'warmup_steps': args.warmup_steps,
            'epochs': args.epochs,
            'seed': args.seed
        },
        'training_history': {
            'train_losses': tracker.train_losses,
            'val_losses': tracker.val_losses,
            'train_perplexities': tracker.train_ppls,
            'val_perplexities': tracker.val_ppls,
            'learning_rates': tracker.learning_rates,
            'gradient_norms': tracker.grad_norms,
            'epoch_times': tracker.step_times
        },
        'convergence_metrics': convergence_metrics,
        'final_results': {
            'total_epochs': len(tracker.train_losses),
            'best_val_loss': min(tracker.val_losses),
            'best_val_perplexity': min(tracker.val_ppls),
            'final_val_loss': tracker.val_losses[-1],
            'final_val_perplexity': tracker.val_ppls[-1],
            'total_training_time': sum(tracker.step_times),
            'average_epoch_time': np.mean(tracker.step_times)
        }
    }

    # ä¿å­˜ JSON ç»“æœ
    results_path = f"{exp_dir}/experiment_results.json"
    with open(results_path, 'w', encoding='utf-8') as f:
        json.dump(results, f, indent=2, ensure_ascii=False)

    # ï¿½ï¿½å­˜æ¨¡å‹
    model_path = f"{exp_dir}/models/final_model.pt"
    torch.save({
        'model_state_dict': model.state_dict(),
        'vocab_size': dataset.vocab_size,
        'char2idx': dataset.char2idx,
        'idx2char': dataset.idx2char,
        'model_config': {
            'd_model': args.d_model,
            'num_heads': args.num_heads,
            'd_ff': args.d_ff,
            'num_layers': args.num_layers,
            'dropout': args.dropout
        }
    }, model_path)

    # ç”Ÿæˆå¹¶ä¿å­˜æ ·æœ¬æ–‡æœ¬
    sample_text = generate_text(model, dataset, torch.device('cuda' if torch.cuda.is_available() else 'cpu'),
                               start_text="First Citizen:", max_len=500, temperature=0.8)

    sample_path = f"{exp_dir}/generated_sample.txt"
    with open(sample_path, 'w', encoding='utf-8') as f:
        f.write("Generated Text Sample:\n")
        f.write("=" * 50 + "\n\n")
        f.write(sample_text)

    print(f" å®éªŒç»“æœå·²ä¿å­˜:")
    print(f"   ç»“æœæ–‡ä»¶: {results_path}")
    print(f"   æ¨¡å‹æ–‡ä»¶: {model_path}")
    print(f"   æ ·æœ¬æ–‡æœ¬: {sample_path}")

    return results


def print_experiment_summary(results):
    """æ‰“å°å®éªŒæ‘˜è¦"""
    print("\n" + "="*80)
    print(" å®éªŒç»“æœæ‘˜è¦")
    print("="*80)

    config = results['experiment_config']
    final = results['final_results']
    convergence = results['convergence_metrics']

    print(f" æ¨¡å‹é…ç½®:")
    print(f"   è¯æ±‡è¡¨å¤§å°: {config['vocab_size']}")
    print(f"   åºåˆ—é•¿åº¦: {config['seq_len']}")
    print(f"   æ¨¡å‹ç»´åº¦: {config['d_model']}")
    print(f"   æ³¨æ„åŠ›å¤´æ•°: {config['num_heads']}")
    print(f"   å±‚æ•°: {config['num_layers']}")

    print(f"\n è®­ç»ƒç»“æœ:")
    print(f"   æ€»è®­ç»ƒè½®æ•°: {final['total_epochs']}")
    print(f"   æœ€ä½³éªŒè¯æŸå¤±: {final['best_val_loss']:.4f}")
    print(f"   æœ€ä½³éªŒè¯å›°æƒ‘åº¦: {final['best_val_perplexity']:.2f}")
    print(f"   æœ€ç»ˆéªŒè¯æŸå¤±: {final['final_val_loss']:.4f}")
    print(f"   æœ€ç»ˆéªŒè¯å›°æƒ‘åº¦: {final['final_val_perplexity']:.2f}")

    print(f"\n  è®­ç»ƒæ—¶é—´:")
    print(f"   æ€»è®­ç»ƒæ—¶é—´: {final['total_training_time']:.1f} ç§’")
    print(f"   å¹³å‡æ¯è½®æ—¶é—´: {final['average_epoch_time']:.1f} ç§’")

    if convergence:
        print(f"\n æ”¶æ•›æ€§åˆ†æ:")
        print(f"   æŸå¤±æ–¹å·®ï¼ˆæœ€è¿‘10è½®ï¼‰: {convergence.get('loss_variance', 'N/A'):.6f}")
        print(f"   æŸå¤±è¶‹åŠ¿ï¼ˆæœ€è¿‘10è½®ï¼‰: {convergence.get('loss_trend', 'N/A'):.6f}")
        print(f"   æŸå¤±å¹³æ»‘æ€§: {convergence.get('loss_smoothness', 'N/A'):.6f}")
        if convergence.get('converged_at_epoch'):
            print(f"   æ”¶æ•›è½®æ•°: {convergence['converged_at_epoch']}")
        else:
            print(f"   çŠ¶æ€: è®­ç»ƒå®Œæˆä½†æœªæ£€æµ‹åˆ°æ”¶æ•›")


def main():
    """ä¸»å®éªŒå‡½æ•°"""
    parser = argparse.ArgumentParser(description='Tiny Shakespeare æ”¶æ•›å®éªŒ')

    # æ¨¡å‹å‚æ•°
    parser.add_argument('--d_model', type=int, default=256, help='æ¨¡å‹ç»´åº¦')
    parser.add_argument('--num_heads', type=int, default=8, help='æ³¨æ„åŠ›å¤´æ•°')
    parser.add_argument('--d_ff', type=int, default=1024, help='å‰é¦ˆç½‘ç»œç»´åº¦')
    parser.add_argument('--num_layers', type=int, default=6, help='Transformer å±‚æ•°')
    parser.add_argument('--dropout', type=float, default=0.1, help='Dropout æ¦‚ç‡')
    parser.add_argument('--seq_len', type=int, default=128, help='åºåˆ—é•¿åº¦')

    # è®­ç»ƒå‚æ•°
    parser.add_argument('--batch_size', type=int, default=64, help='æ‰¹æ¬¡å¤§å°')
    parser.add_argument('--epochs', type=int, default=50, help='è®­ç»ƒè½®æ•°')
    parser.add_argument('--lr', type=float, default=3e-4, help='å­¦ä¹ ç‡')
    parser.add_argument('--weight_decay', type=float, default=0.01, help='æƒé‡è¡°å‡')
    parser.add_argument('--warmup_steps', type=int, default=2000, help='Warmup æ­¥æ•°')

    # å®éªŒå‚æ•°
    parser.add_argument('--seed', type=int, default=42, help='éšæœºç§å­')
    parser.add_argument('--patience', type=int, default=10, help='æ—©åœè€å¿ƒå€¼')
    parser.add_argument('--min_delta', type=float, default=1e-4, help='æœ€å°æ”¹è¿›é˜ˆå€¼')

    args = parser.parse_args()

    # è®¾ç½®å®éªŒç¯å¢ƒ
    device, exp_dir = setup_experiment(args)

    # åŠ è½½æ•°æ®
    train_dataset, val_dataset, train_loader, val_loader = load_shakespeare_data(args)

    # åˆ›å»ºæ¨¡å‹
    model = create_model(train_dataset.vocab_size, args, device)

    # è®¾ç½®è®­ç»ƒç»„ä»¶
    args.steps_per_epoch = len(train_loader)
    optimizer, criterion, scheduler = setup_training(model, args)

    # åˆå§‹åŒ–æ”¶æ•›è·Ÿè¸ªå™¨
    tracker = ConvergenceTracker(patience=args.patience, min_delta=args.min_delta)

    print(f"\n å¼€å§‹è®­ç»ƒ")
    print("-" * 50)

    best_val_loss = float('inf')

    # è®­ç»ƒå¾ªç¯
    for epoch in range(1, args.epochs + 1):
        # è®­ç»ƒ
        train_loss, train_ppl, grad_norm, epoch_time = train_epoch(
            model, train_loader, optimizer, criterion, device, epoch, tracker, scheduler
        )

        # éªŒè¯
        val_loss, val_ppl = validate(model, val_loader, criterion, device)

        # è·å–å½“å‰å­¦ä¹ ç‡
        current_lr = optimizer.param_groups[0]['lr']

        # æ›´æ–°è·Ÿè¸ªå™¨
        early_stop = tracker.update(train_loss, val_loss, train_ppl, val_ppl,
                                   current_lr, grad_norm, epoch_time)

        # æ‰“å°è¿›åº¦
        print(f"\nEpoch {epoch:3d}/{args.epochs}")
        print(f"  Train Loss: {train_loss:.4f} | Train PPL: {train_ppl:7.2f}")
        print(f"  Val Loss:   {val_loss:.4f} | Val PPL:   {val_ppl:7.2f}")
        print(f"  LR: {current_lr:.2e} | Grad Norm: {grad_norm:.3f} | Time: {epoch_time:.1f}s")

        # ä¿å­˜æœ€ä½³æ¨¡å‹
        if val_loss < best_val_loss:
            best_val_loss = val_loss
            best_model_path = f"{exp_dir}/models/best_model.pt"
            torch.save({
                'epoch': epoch,
                'model_state_dict': model.state_dict(),
                'optimizer_state_dict': optimizer.state_dict(),
                'val_loss': val_loss,
                'args': args
            }, best_model_path)
            print(f"   æœ€ä½³æ¨¡å‹å·²ä¿å­˜!")

        # æ¯10è½®ç”Ÿæˆæ ·æœ¬æ–‡æœ¬
        if epoch % 10 == 0:
            print(f"\n ç”Ÿæˆæ ·æœ¬ (Epoch {epoch}):")
            sample = generate_text(model, train_dataset, device,
                                 start_text="ROMEO:", max_len=150, temperature=0.8)
            print(f"   {sample[:100]}...")

        # æ£€æŸ¥æ—©åœ
        if early_stop:
            print(f"\n  Early stopping at epoch {epoch}")
            print(f"    No improvement for {args.patience} epochs")
            break

    # ç»˜åˆ¶å’Œä¿å­˜ç»“æœ
    plot_convergence_analysis(tracker, exp_dir)
    results = save_experiment_results(args, tracker, model, train_dataset, exp_dir)
    print_experiment_summary(results)

    print(f"\n å®éªŒå®Œæˆï¼ç»“æœä¿å­˜åœ¨: {exp_dir}")


if __name__ == "__main__":
    main()